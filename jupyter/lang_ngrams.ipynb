{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf1e8d1-b0ed-4acb-a508-334fd43ff14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tqdm \n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "import string \n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd8e1b5-30f2-4b38-a0d3-4eb8ffed3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../datasets/lang_det/all.txt\", \"r\") as file :\n",
    "    eng_fr_dataset_v1  = file.readlines()\n",
    "\n",
    "with open (\"../datasets/tunisien/scrapped_texts/comments.txt\", \"r\") as file :\n",
    "    tn_dataset_v1  = file.readlines()\n",
    "\n",
    "with open (\"../datasets/tunisien/scrapped_texts/messages.txt\", \"r\") as file :\n",
    "    tn_dataset_v2  = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa02d57-58bd-46fb-9398-e7eee1f3c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = str.maketrans(string.punctuation,\" \"*(len(string.punctuation)))\n",
    "\n",
    "def transform_text(txt:str):\n",
    "    _txt = txt\n",
    "    _txt = _txt.translate(trans)\n",
    "    _txt = emoji.get_emoji_regexp().sub(\"\", _txt)\n",
    "    _txt = _txt.strip().lower()\n",
    "    _txt = re.sub(\"\\s+\", \" \", _txt)\n",
    "    return _txt\n",
    "\n",
    "def parse_train(line:str, transform=False):\n",
    "    _label = re.compile(\"__label__[\\w]{3}\").search(line).group()\n",
    "    _text = line.replace(_label,\"\")\n",
    "    if transform:\n",
    "        _text = transform_text(_text)\n",
    "    \n",
    "    return (_text, _label)\n",
    "\n",
    "def parse_unlabled_train(line:str,label=\"__label__tun\"):    \n",
    "    return (transform_text(line), label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fec89f6-ab07-4f36-a8cf-3d0c67095a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3274302/3274302 [00:04<00:00, 795512.29it/s]\n"
     ]
    }
   ],
   "source": [
    "eng_fr_dataset_v1 = list(map(lambda x: parse_train(x),tqdm.tqdm(eng_fr_dataset_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dff13e4-90c3-4e72-a56e-8fa58f69b102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63176/63176 [00:15<00:00, 4099.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tn_dataset_v1 = list(map(lambda x: parse_unlabled_train(x),tqdm.tqdm(tn_dataset_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fc5679-13ae-4360-871b-37379903aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76122/76122 [00:09<00:00, 7801.26it/s]\n"
     ]
    }
   ],
   "source": [
    "tn_dataset_v2 = list(map(lambda x: parse_unlabled_train(x),tqdm.tqdm(tn_dataset_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccfd0d00-35ec-4b9e-ac88-94ca2fedfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = []\n",
    "fr = []\n",
    "\n",
    "for line in eng_fr_dataset_v1 :\n",
    "    if line[1]==\"__label__eng\":\n",
    "        en.append(line)\n",
    "    else :\n",
    "        fr.append(line)\n",
    "\n",
    "tn = []\n",
    "tn.extend(tn_dataset_v1)\n",
    "tn.extend(tn_dataset_v2)\n",
    "\n",
    "tn_texts= [text[0] for text in tn]\n",
    "fr_texts = [text[0] for text in fr[:300000]]\n",
    "en_texts= [text[0] for text in en[:300000]]\n",
    "\n",
    "all_texts = []\n",
    "all_texts.extend(tn_texts)\n",
    "all_texts.extend(fr_texts)\n",
    "all_texts.extend(en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299e6b92-967e-4621-9371-8f1d1a796b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_vectorizer = CountVectorizer(analyzer=\"char_wb\", ngram_range=(2,3), min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b67e5bc-0f43-41a2-9849-d15ace51eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char_wb', min_df=2, ngram_range=(2, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn_vectorizer.fit(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c70daafa-27e5-4881-9e1f-7acbf76fc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorizer.bin\",\"wb\") as file:\n",
    "    pickle.dump(tn_vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bf9a0-881a-4b25-8c3b-855fb4dabd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc26490-95ba-49ba-b0ac-f36dc7fb5d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a381c030-bf1a-4cf0-893d-a53d051a50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(txt, max_length=12):\n",
    "    _txt = txt.split()\n",
    "    \n",
    "    random_length = np.random.randint(1,max_length+1)\n",
    "    return \" \".join(_txt[:random_length])\n",
    "\n",
    "\n",
    "def featurize(arr:[(str,str)], max_length=12):\n",
    "    _texts = []\n",
    "    _labels = []\n",
    "    \n",
    "    label2id={\"__label__fra\":0,\"__label__eng\":1, \"__label__tun\":2 }\n",
    "    id2label={0:\"__label__fra\",1:\"__label__eng\", 2:\"__label__tun\"}\n",
    "    \n",
    "    for t,l in arr:\n",
    "        _texts.append(extract_text(t,max_length=max_length))\n",
    "        _labels.append(label2id[l])\n",
    "    \n",
    "    _features = tn_vectorizer.transform(_texts).toarray()\n",
    "    \n",
    "    return _features, np.array(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33b731af-283d-4057-ba1c-b4956d9e9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(iterator, batch_size = 32) :\n",
    "    return len(iterator)//batch_size\n",
    "\n",
    "\n",
    "def generator(iterator, batch_size = 32, max_length= 12, steps=None, epochs=1):\n",
    "    \n",
    "    \n",
    "    _len = len(iterator)\n",
    "    \n",
    "    for e in range(epochs) :\n",
    "        if steps is None:\n",
    "            for i  in range(0,_len,batch_size) :\n",
    "                data = iterator[i:i+batch_size]\n",
    "                yield featurize(data, max_length)\n",
    "        else :\n",
    "            for i in range(steps):\n",
    "                indexes = np.random.choice(range(_len),batch_size, replace=False)\n",
    "                data =  [iterator[ind] for ind in  indexes]\n",
    "                yield featurize(data, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "188101e1-1c55-442e-aff5-bdc47a1eec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "min_length = min(len(en), len(fr), len(tn))\n",
    "\n",
    "np.random.shuffle(tn)\n",
    "np.random.shuffle(en)\n",
    "np.random.shuffle(fr)\n",
    "\n",
    "tn = tn[:min_length-1]\n",
    "fr = fr[:min_length-1]\n",
    "en = en[:min_length-1]\n",
    "\n",
    "\n",
    "train.extend(tn)\n",
    "train.extend(en)\n",
    "train.extend(fr)\n",
    "\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cee7a6e-b7e6-4811-94e2-71ad60daba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(tn_vectorizer.vocabulary_)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_dim),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(512,activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Dense(3,activation=\"softmax\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d750409-d504-478c-94d3-1199496efe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 28276)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              28955648  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 29,488,131\n",
      "Trainable params: 29,485,059\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44a2723-29c3-40c6-86bf-6e9e6580da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f116d1a-9541-44dd-92fc-09b3f5d03d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83c796e8-f4bb-474a-8d9b-045f84aa24c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "979"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "max_length=12\n",
    "train_steps =  400\n",
    "test_steps = steps(X_test, batch_size)\n",
    "steps(X_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df2f4a39-7bce-4ded-8e0a-10b859a8399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callbacks = callbacks.TensorBoard(\"./logs_lang_detector\")\n",
    "early_callback = callbacks.EarlyStopping(patience=5)\n",
    "lr_callback= callbacks.ReduceLROnPlateau(patience=2)\n",
    "save_callback = callbacks.ModelCheckpoint(\"model_lang\", save_best_only=True)\n",
    "\n",
    "clbk = [tensorboard_callbacks, early_callback,lr_callback,save_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c83d351-6198-484f-88f3-9ebc407ca087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = generator(X_train, batch_size, max_length, steps=train_steps, epochs=epochs)\n",
    "X_test = generator(X_test, batch_size, max_length,steps=test_steps,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a86c0662-fdbb-49f3-9c1a-0a861c27264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "400/400 [==============================] - 73s 183ms/step - loss: 0.2575 - accuracy: 0.9095 - val_loss: 0.0968 - val_accuracy: 0.9705\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 2/30\n",
      "400/400 [==============================] - 70s 175ms/step - loss: 0.1178 - accuracy: 0.9578 - val_loss: 0.0758 - val_accuracy: 0.9752\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 3/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.1066 - accuracy: 0.9654 - val_loss: 0.0692 - val_accuracy: 0.9774\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 4/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0956 - accuracy: 0.9681 - val_loss: 0.0640 - val_accuracy: 0.9782\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 5/30\n",
      "400/400 [==============================] - 65s 162ms/step - loss: 0.0857 - accuracy: 0.9725 - val_loss: 0.0621 - val_accuracy: 0.9792\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 6/30\n",
      "400/400 [==============================] - 65s 163ms/step - loss: 0.0892 - accuracy: 0.9706 - val_loss: 0.0655 - val_accuracy: 0.9791\n",
      "Epoch 7/30\n",
      "400/400 [==============================] - 66s 166ms/step - loss: 0.0835 - accuracy: 0.9717 - val_loss: 0.0616 - val_accuracy: 0.9805\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 8/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0777 - accuracy: 0.9726 - val_loss: 0.0600 - val_accuracy: 0.9809\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 9/30\n",
      "400/400 [==============================] - 68s 170ms/step - loss: 0.0798 - accuracy: 0.9736 - val_loss: 0.0592 - val_accuracy: 0.9808\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 10/30\n",
      "400/400 [==============================] - 65s 163ms/step - loss: 0.0813 - accuracy: 0.9740 - val_loss: 0.0590 - val_accuracy: 0.9809\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 11/30\n",
      "400/400 [==============================] - 65s 164ms/step - loss: 0.0796 - accuracy: 0.9746 - val_loss: 0.0536 - val_accuracy: 0.9822\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 12/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0722 - accuracy: 0.9759 - val_loss: 0.0559 - val_accuracy: 0.9815\n",
      "Epoch 13/30\n",
      "400/400 [==============================] - 66s 166ms/step - loss: 0.0749 - accuracy: 0.9748 - val_loss: 0.0587 - val_accuracy: 0.9817\n",
      "Epoch 14/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0701 - accuracy: 0.9753 - val_loss: 0.0502 - val_accuracy: 0.9839\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 15/30\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0635 - accuracy: 0.9787 - val_loss: 0.0491 - val_accuracy: 0.9836\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 16/30\n",
      "400/400 [==============================] - 65s 162ms/step - loss: 0.0596 - accuracy: 0.9794 - val_loss: 0.0475 - val_accuracy: 0.9843\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 17/30\n",
      "400/400 [==============================] - 72s 181ms/step - loss: 0.0630 - accuracy: 0.9796 - val_loss: 0.0512 - val_accuracy: 0.9831\n",
      "Epoch 18/30\n",
      "400/400 [==============================] - 71s 178ms/step - loss: 0.0597 - accuracy: 0.9795 - val_loss: 0.0491 - val_accuracy: 0.9838\n",
      "Epoch 19/30\n",
      "400/400 [==============================] - 73s 183ms/step - loss: 0.0642 - accuracy: 0.9781 - val_loss: 0.0469 - val_accuracy: 0.9846\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 20/30\n",
      "400/400 [==============================] - 73s 182ms/step - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.0465 - val_accuracy: 0.9842\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 21/30\n",
      "400/400 [==============================] - 76s 190ms/step - loss: 0.0667 - accuracy: 0.9783 - val_loss: 0.0456 - val_accuracy: 0.9846\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 22/30\n",
      "400/400 [==============================] - 71s 177ms/step - loss: 0.0611 - accuracy: 0.9802 - val_loss: 0.0481 - val_accuracy: 0.9832\n",
      "Epoch 23/30\n",
      "400/400 [==============================] - 73s 183ms/step - loss: 0.0586 - accuracy: 0.9805 - val_loss: 0.0490 - val_accuracy: 0.9837\n",
      "Epoch 24/30\n",
      "400/400 [==============================] - 69s 172ms/step - loss: 0.0621 - accuracy: 0.9789 - val_loss: 0.0453 - val_accuracy: 0.9847\n",
      "INFO:tensorflow:Assets written to: model_lang/assets\n",
      "Epoch 25/30\n",
      "400/400 [==============================] - 71s 178ms/step - loss: 0.0650 - accuracy: 0.9791 - val_loss: 0.0469 - val_accuracy: 0.9836\n",
      "Epoch 26/30\n",
      "400/400 [==============================] - 74s 186ms/step - loss: 0.0599 - accuracy: 0.9810 - val_loss: 0.0460 - val_accuracy: 0.9844\n",
      "Epoch 27/30\n",
      "400/400 [==============================] - 74s 184ms/step - loss: 0.0634 - accuracy: 0.9802 - val_loss: 0.0461 - val_accuracy: 0.9842\n",
      "Epoch 28/30\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 0.0611 - accuracy: 0.9799 - val_loss: 0.0471 - val_accuracy: 0.9840\n",
      "Epoch 29/30\n",
      "400/400 [==============================] - 83s 209ms/step - loss: 0.0604 - accuracy: 0.9812 - val_loss: 0.0456 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb927589e50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, epochs=epochs, validation_data=X_test, steps_per_epoch=train_steps, validation_steps=test_steps, callbacks=clbk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405d3f5-ca0c-482d-89b7-e62e94c12f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d891948-4972-4d8c-85ac-4c5b02acc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang(txt:str, model, vectorizer,label_dict):\n",
    "    _txt = transform_text(txt)\n",
    "    _txt = vectorizer.transform([_txt]).toarray()\n",
    "    res = model(_txt)[0]\n",
    "    print(res)\n",
    "    res =  tf.argmax(res).numpy()\n",
    "    return label_dict[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a33503a-50e6-483c-98d2-10e55bc9a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./model_lang/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70838ff0-1746-4215-b3f4-7a15276b57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./vectorizer.bin\", \"rb\") as file:\n",
    "    vectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e5cc958-934a-4a9a-8f9b-15cd39cb0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.53562284 0.08138265 0.38299447], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__label__fra'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lang(\"non\", model, vectorizer, {0:\"__label__fra\",1:\"__label__eng\", 2:\"__label__tun\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b14eb-478f-4b5d-b56a-a370ad207fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
