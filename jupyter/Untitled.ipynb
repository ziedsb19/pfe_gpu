{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f742e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers as tr\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer, BertWordPieceTokenizer\n",
    "from tokenizers import pre_tokenizers\n",
    "import os\n",
    "import re\n",
    "import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575ac27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../datasets/lang_det/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9817bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (os.path.join(dataset_dir,\"all.txt\"), \"r\") as file :\n",
    "    eng_fr_dataset_v1  = file.readlines()\n",
    "\n",
    "with open (os.path.join(dataset_dir,\"all_text_lang.txt\"), \"r\") as file :\n",
    "    tn_dataset_v1  = file.readlines()\n",
    "    \n",
    "with open (os.path.join(dataset_dir,\"messages.txt\"), \"r\") as file :\n",
    "    tn_dataset_v2  = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedc00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train(line:str):\n",
    "    _label = re.compile(\"__label__[\\w]{3}\").search(line).group()\n",
    "    _text = line.replace(_label,\"\")\n",
    "    \n",
    "    return (_text, _label)\n",
    "\n",
    "def parse_unlabled_train(line:str,label=\"__label__tun\"):    \n",
    "    return (line, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf79f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3274302/3274302 [00:04<00:00, 772434.23it/s]\n"
     ]
    }
   ],
   "source": [
    "eng_fr_dataset_v1 = list(map(lambda x: parse_train(x),tqdm.tqdm(eng_fr_dataset_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97c6820-2057-4a9e-9afc-bae58df50719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 800951.75it/s]\n"
     ]
    }
   ],
   "source": [
    "tn_dataset_v1 = list(map(lambda x: parse_train(x),tqdm.tqdm(tn_dataset_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635596c9-c566-4c10-8f9a-d90a1863e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76122/76122 [00:00<00:00, 2759254.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tn_dataset_v2 = list(map(lambda x: parse_unlabled_train(x),tqdm.tqdm(tn_dataset_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09afa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = []\n",
    "fr = []\n",
    "\n",
    "for line in eng_fr_dataset_v1 :\n",
    "    if line[1]==\"__label__eng\":\n",
    "        en.append(line)\n",
    "    else :\n",
    "        fr.append(line)\n",
    "\n",
    "tn = []\n",
    "tn.extend(tn_dataset_v1)\n",
    "tn.extend(tn_dataset_v2)\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "all_texts.extend(en)\n",
    "all_texts.extend(fr)\n",
    "all_texts.extend(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75608175-2513-4363-aeab-5a121b0f0e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(tn,columns=[\"text\",\"label\"])[\"text\"].to_csv(\"tun.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefffda9-5a1e-4f4b-ae24-3b83f7977ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer  = BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36f1ba2-b08a-42f0-8c7e-b313011b2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.train_from_iterator(all_texts, vocab_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc5c4194-4154-4d0a-9946-2f34dad172dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waa', '##a']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.encode(\"\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cdec3-4d63-4e36-945f-87898c102069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50ca19-88a9-4012-8da9-5225a10e257e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6404d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "min_length = min(len(en), len(fr), len(tn))\n",
    "\n",
    "np.random.shuffle(tn)\n",
    "np.random.shuffle(en)\n",
    "np.random.shuffle(fr)\n",
    "\n",
    "tn = tn[:min_length-1]\n",
    "fr = fr[:min_length-1]\n",
    "en = en[:min_length-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e67cd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.extend(tn)\n",
    "train.extend(en)\n",
    "train.extend(fr)\n",
    "\n",
    "np.random.shuffle(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03510591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Behy haw bech nab3athlek lien mta3 eli ena t3a...</td>\n",
       "      <td>__label__tun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My younger brother has a lot of money.\\n</td>\n",
       "      <td>__label__eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J'ai vu ce matin une jolie rue dont j'ai oubl...</td>\n",
       "      <td>__label__fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sami told Layla about the whole thing with Sa...</td>\n",
       "      <td>__label__eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allah yar7mou w y na3mou w yaj3al mathweh el ...</td>\n",
       "      <td>__label__tun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528358</th>\n",
       "      <td>U.S. District Judge Joan Ericksen ruled in th...</td>\n",
       "      <td>__label__eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528359</th>\n",
       "      <td>Behi n5amem we n9olek 5ater 3andi jamia fiha m...</td>\n",
       "      <td>__label__tun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528360</th>\n",
       "      <td>hmdlh 8dartna f s5ana hhhhh\\n</td>\n",
       "      <td>__label__tun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528361</th>\n",
       "      <td>La neige est tombée sans discontinuer samedi ...</td>\n",
       "      <td>__label__fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528362</th>\n",
       "      <td>This horse hasn't been ridden in weeks.\\n</td>\n",
       "      <td>__label__eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text         label\n",
       "0       Behy haw bech nab3athlek lien mta3 eli ena t3a...  __label__tun\n",
       "1                My younger brother has a lot of money.\\n  __label__eng\n",
       "2        J'ai vu ce matin une jolie rue dont j'ai oubl...  __label__fra\n",
       "3        Sami told Layla about the whole thing with Sa...  __label__eng\n",
       "4        allah yar7mou w y na3mou w yaj3al mathweh el ...  __label__tun\n",
       "...                                                   ...           ...\n",
       "528358   U.S. District Judge Joan Ericksen ruled in th...  __label__eng\n",
       "528359  Behi n5amem we n9olek 5ater 3andi jamia fiha m...  __label__tun\n",
       "528360                      hmdlh 8dartna f s5ana hhhhh\\n  __label__tun\n",
       "528361   La neige est tombée sans discontinuer samedi ...  __label__fra\n",
       "528362          This horse hasn't been ridden in weeks.\\n  __label__eng\n",
       "\n",
       "[528363 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(train, columns=[\"text\",\"label\"])\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73a63163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__label__fra    176121\n",
       "__label__tun    176121\n",
       "__label__eng    176121\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7fb9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = tr.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac11070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ae0e3-7cb7-4da8-b831-cb8b9702dc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a978943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del en, fr,  eng_fr_dataset_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "460598c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label2id={\"__label__fra\":0,\"__label__eng\":1,\"__label__tun\":2}\n",
    "#id2label={0:\"__label__fra\",1:\"__label__eng\",2:\"__label__tun\"}\n",
    "\n",
    "label2id={\"__label__fra\":0,\"__label__eng\":1, \"__label__tun\":2 }\n",
    "id2label={0:\"__label__fra\",1:\"__label__eng\", 2:\"__label__tun\"}\n",
    "\n",
    "\n",
    "def tokenize(lines:str, max_length=16):\n",
    "    if max_length is None :\n",
    "        max_length = 16\n",
    "    random_length = np.random.randint(1,max_length+1)\n",
    "    \n",
    "    text, label = lines[0], label2id[ lines[1] ]\n",
    "    input_ids = bert_tokenizer.encode(text).ids[:random_length+1]\n",
    "    \n",
    "    return (input_ids, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ca07533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_array(lines, max_length=16):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for item in lines :\n",
    "        fe,lab = tokenize(item, max_length)\n",
    "        features.append(fe)\n",
    "        labels.append(lab)\n",
    "    if max_length is None :\n",
    "        max_length = max(list(map(lambda x: len(x), features)))\n",
    "   \n",
    "    len_features = len(features)\n",
    "    \n",
    "    input_ids =  np.zeros((len_features, max_length))\n",
    "    \n",
    "    for index, item in enumerate(features) :\n",
    "        input_ids[index][:min(len(item),max_length)] = item[:min(len(item),max_length)]\n",
    "    \n",
    "    return np.array(input_ids) ,  np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "161e1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(iterator, batch_size = 32) :\n",
    "    return len(iterator)//batch_size\n",
    "\n",
    "\n",
    "def generator(iterator, batch_size = 32, max_length= 16, steps=None, epochs=1):\n",
    "    \n",
    "    \n",
    "    _len = len(iterator)\n",
    "    \n",
    "    for e in range(epochs) :\n",
    "        if steps is None:\n",
    "            for i  in range(0,_len,batch_size) :\n",
    "                data = iterator[i:i+batch_size]\n",
    "                yield tokenize_array(data, max_length)\n",
    "        else :\n",
    "            for i in range(steps):\n",
    "                indexes = np.random.choice(range(_len),batch_size, replace=False)\n",
    "                data =  [iterator[ind] for ind in  indexes]\n",
    "                yield tokenize_array(data, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051965d-0889-4a4b-b107-38147b1f4a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1a137cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = bert_tokenizer.get_vocab_size()\n",
    "embedding_size = 128\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim,  output_dim=embedding_size),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(512,activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    #keras.layers.GRU(512, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    #keras.layers.GRU(512, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(3,activation=\"softmax\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98486d4-5b97-4251-81bf-8fbef1a0cc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c55c239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         6400000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 7,065,091\n",
      "Trainable params: 7,061,763\n",
      "Non-trainable params: 3,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f702c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ad2f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5b96188",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 16\n",
    "max_length=16\n",
    "train_steps =  200\n",
    "test_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15d9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = generator(X_train, batch_size, max_length, steps=train_steps, epochs=epochs)\n",
    "X_test = generator(X_test, batch_size, max_length,steps=test_steps,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d2b4e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 21s 100ms/step - loss: 0.9930 - accuracy: 0.6512 - val_loss: 1.0322 - val_accuracy: 0.3309\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.3832 - accuracy: 0.8692 - val_loss: 0.5627 - val_accuracy: 0.8878\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.3311 - accuracy: 0.8868 - val_loss: 0.2251 - val_accuracy: 0.9287\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 0.2670 - accuracy: 0.9138 - val_loss: 0.3671 - val_accuracy: 0.8669\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 0.2548 - accuracy: 0.9129 - val_loss: 0.1455 - val_accuracy: 0.9503\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.2143 - accuracy: 0.9213 - val_loss: 0.1439 - val_accuracy: 0.9506\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.2448 - accuracy: 0.9127 - val_loss: 0.1490 - val_accuracy: 0.9525\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.2132 - accuracy: 0.9215 - val_loss: 0.1832 - val_accuracy: 0.9394\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.2228 - accuracy: 0.9188 - val_loss: 0.1215 - val_accuracy: 0.9591\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 0.1888 - accuracy: 0.9366 - val_loss: 0.1081 - val_accuracy: 0.9631\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 0.1803 - accuracy: 0.9385 - val_loss: 0.1091 - val_accuracy: 0.9609\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 26s 133ms/step - loss: 0.1817 - accuracy: 0.9330 - val_loss: 0.1223 - val_accuracy: 0.9572\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1783 - accuracy: 0.9376 - val_loss: 0.1183 - val_accuracy: 0.9625\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 25s 125ms/step - loss: 0.1881 - accuracy: 0.9408 - val_loss: 0.1985 - val_accuracy: 0.9244\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 25s 127ms/step - loss: 0.1424 - accuracy: 0.9497 - val_loss: 0.1196 - val_accuracy: 0.9619\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 0.1609 - accuracy: 0.9485 - val_loss: 0.0993 - val_accuracy: 0.9672\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 0.1984 - accuracy: 0.9388 - val_loss: 0.1072 - val_accuracy: 0.9653\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 0.1712 - accuracy: 0.9491 - val_loss: 0.0917 - val_accuracy: 0.9706\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 0.1649 - accuracy: 0.9399 - val_loss: 0.1283 - val_accuracy: 0.9572\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 0.1656 - accuracy: 0.9440 - val_loss: 0.1161 - val_accuracy: 0.9553\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.1517 - accuracy: 0.9463 - val_loss: 0.0987 - val_accuracy: 0.9641\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 0.1701 - accuracy: 0.9425 - val_loss: 0.0927 - val_accuracy: 0.9659\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.1509 - accuracy: 0.9486 - val_loss: 0.1146 - val_accuracy: 0.9638\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.1412 - accuracy: 0.9504 - val_loss: 0.0975 - val_accuracy: 0.9638\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.1491 - accuracy: 0.9484 - val_loss: 0.1252 - val_accuracy: 0.9556\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.1295 - accuracy: 0.9550 - val_loss: 0.1624 - val_accuracy: 0.9463\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.1669 - accuracy: 0.9468 - val_loss: 0.0898 - val_accuracy: 0.9709\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.1347 - accuracy: 0.9515 - val_loss: 0.0946 - val_accuracy: 0.9653\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 0.1752 - accuracy: 0.9491 - val_loss: 0.0829 - val_accuracy: 0.9725\n",
      "Epoch 30/30\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9526WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 0.1495 - accuracy: 0.9526 - val_loss: 0.1149 - val_accuracy: 0.9638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe434b62460>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, epochs=epochs, validation_data=X_test, steps_per_epoch=train_steps, validation_steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4ee32873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[7.362371e-07, 9.990735e-01, 9.257449e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tokens = tf.expand_dims( bert_tokenizer.encode(\"are you doing fine ?\").ids , 0)\n",
    "model(_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e80ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef69617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d4864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab7b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb55d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
